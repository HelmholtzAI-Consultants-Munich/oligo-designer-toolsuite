{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padlock Probe Designer\n",
    "\n",
    "This notebook implements a pipeline to designe Padlock probes (short oligos) using the `oligo_designer_toolbox` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necassary pakages exept from the oligo_designer_toolsuite\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import yaml\n",
    "from Bio.SeqUtils import MeltingTemp as mt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the parameters\n",
    "\n",
    "The first thing to do is to define the parameters we want to use to generate the Padlock probes. \n",
    "A possible way to define all the parameters, that is flexible and reusable, is to use a configuration file. \n",
    "For this toutorial we will use the YAMl file `padlock_probe_designer_custom.yaml`, which uses a custom gene annotation. You can have a look to understand which parameters are required and how the configuration file is structured.\n",
    "If you want to use NCBI or Ensembl gene annotation with automatic annotation download from their servers, please check out the YAMl files `padlock_probe_designer_ncbi.yaml` and `padlock_probe_designer_ensembl.yaml`.\n",
    "\n",
    "Once the configuration file has been set up we have to read its content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"./configs/padlock_probe_designer_custom.yaml\"\n",
    "with open(config_file, 'r') as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)\n",
    "dir_output = os.path.join(os.path.dirname(os.getcwd()), config[\"dir_output\"]) # create the complete path for the output directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oligo sequences generation\n",
    "\n",
    "Now we can start to actually build the pipeline, we will start by generating all the possible oligos with length between the maximum and minimum value given belonging to the genes defined in the config file. The oligos will be saved in a nested dicionary with the following structure: \n",
    "\n",
    "[gene][oligo_id][oligo_feature].\n",
    "\n",
    "\n",
    "For this we need a `CustomOligoDB`, or a class that inherits form it (e.g. `NCBIOligoDB` and `EnsemblOligoDB`) and call the method `create_oligos_DB`. These classes differ on how the the fasta and the gtf filesused to compute the sequences are obtained. The first one oses local files while the others dowload them form the NCBI or Ensambl ftp server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oligo_designer_toolsuite.database import CustomOligoDB, NcbiOligoDB, EnsemblOligoDB\n",
    "\n",
    "# define the database class\n",
    "if config[\"source\"] == \"ncbi\":\n",
    "    # dowload the fasta files formthe NCBI server\n",
    "    oligo_database = NcbiOligoDB(\n",
    "        oligo_length_min=config[\"oligo_length_min\"],\n",
    "        oligo_length_max=config[\"oligo_length_max\"],\n",
    "        species=config[\"species\"],\n",
    "        annotation_release=config[\"annotation_release\"],\n",
    "        n_jobs=config[\"n_jobs\"],\n",
    "        dir_output=dir_output,\n",
    "        min_oligos_per_gene=config[\"min_oligos_per_gene\"],\n",
    "        )\n",
    "elif config[\"source\"] == \"ensembl\":\n",
    "    # dowload the fasta files formthe NCBI server\n",
    "    oligo_database = EnsemblOligoDB(\n",
    "        oligo_length_min=config[\"oligo_length_min\"],\n",
    "        oligo_length_max=config[\"oligo_length_max\"],\n",
    "        species=config[\"species\"],\n",
    "        annotation_release=config[\"annotation_release\"],\n",
    "        n_jobs=config[\"n_jobs\"],\n",
    "        dir_output=dir_output,\n",
    "        min_oligos_per_gene=config[\"min_oligos_per_gene\"],\n",
    "        )\n",
    "elif config[\"source\"] == \"custom\":\n",
    "    # use already dowloaded files\n",
    "    oligo_database = CustomOligoDB(\n",
    "        oligo_length_min=config[\"oligo_length_min\"],\n",
    "        oligo_length_max=config[\"oligo_length_max\"],\n",
    "        species=config[\"species\"],\n",
    "        genome_assembly=config[\"genome_assembly\"],\n",
    "        annotation_release=config[\"annotation_release\"],\n",
    "        files_source=config[\"files_source\"],\n",
    "        annotation_file=config[\"annotation_file\"],\n",
    "        sequence_file=config[\"sequence_file\"],\n",
    "        n_jobs=config[\"n_jobs\"],\n",
    "        dir_output=dir_output,\n",
    "        min_oligos_per_gene=config[\"min_oligos_per_gene\"],\n",
    "        )\n",
    "else:\n",
    "    raise ValueError(\"Annotation source not supported!\") \n",
    "\n",
    "# read the genes file\n",
    "with open(config[\"file_genes\"]) as handle:\n",
    "    lines = handle.readlines()\n",
    "    genes = [line.rstrip() for line in lines]\n",
    "    \n",
    "#generate the oligo sequences from gene transcripts\n",
    "oligo_database.create_oligos_DB(genes=genes, region='transcripts')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary structure\n",
    "\n",
    "Here is an example of how the nested dictionary is structured. We print a very simple dictionary with 1 oligo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CYBA': {'CYBA_1': {'sequence': Seq('ACAGCTGGGCGCTTCACCCAGTGGTACTTTGGTGCCTA'), 'transcript_id': ['NM_000101.4:XM_011522905.4', 'NM_000101.4:XM_011522905.4'], 'exon_id': ['NM_000101.4_exon3_NM_000101.4_exon2:XM_011522905.4_exon3_XM_011522905.4_exon2', 'NM_000101.4_exon2:XM_011522905.4_exon2'], 'chromosome': '16', 'start': [88647131, 88648070], 'end': [88647169, 88648108], 'strand': '-', 'length': 38}}}\n"
     ]
    }
   ],
   "source": [
    "gene = list(oligo_database.oligos_DB.keys())[0]\n",
    "oligo_id = list(oligo_database.oligos_DB[gene].keys())[0]\n",
    "\n",
    "sample_oligos_DB = {}\n",
    "sample_oligos_DB[gene] = {}\n",
    "sample_oligos_DB[gene][oligo_id] = oligo_database.oligos_DB[gene][oligo_id]\n",
    "print(sample_oligos_DB)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and write\n",
    "\n",
    "These classes deal with everything that is related with the management of the dataset. In particular, beyond creatig the dataset, they can also read and write the oligo sequences in a **tsv** or **gtf** fromat. The methods `read_oligos_DB` and `write_oligos_DB` have exactly this purpose.\n",
    "\n",
    "Therefore, it is possible to save the current state of the dictionary during the pipeline and to retrive form a previous stage if an error uccurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"write_intermediate_steps\"]:\n",
    "    oligo_database.write_oligos_DB(format=config[\"file_format\"], dir_oligos_DB=\"oligos_creation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property filters\n",
    "\n",
    "Once all the possible sequences are created, we apply a first filtering process based on the sequences properties (e.g. melting temperature or GC content). This is useful to reduce the amount of sequences we have to deal with in the next stages and discard all the sequences that are not suited for the experiment.\n",
    "\n",
    "Each property filter is a class that inherits from the Abstact Base Class `PreFilterBase` They have a method called `apply` that takes the `oligos_DB` dictionary and returns it filtered. To make this process smooth and modular the class `PropertyFilter` allows to apply several filters one after the other. It takes as input a list of filter classes and a `CustomOligoDB` and applies sequentially all the filters and returns the final filterd version of the database. Additionally, all the necessary sequence features computed by the filters are stored in the `oligos_DB` for possible later use. \n",
    "\n",
    "*Note:* the filters are applied in the order they are given as input. Hence, filter with fast computations should be listed first, i.e. apply GC content filter before melting temperature filter, to reduce runtime.\n",
    "\n",
    "To create new property filters follow the Abstact Base Class requirements in `PreFilterBase`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oligo_designer_toolsuite.oligo_property_filter import (\n",
    "    PropertyFilter,\n",
    "    MaskedSequences,\n",
    "    GCContent, \n",
    "    MeltingTemperature, \n",
    "    PadlockArms\n",
    ")\n",
    "\n",
    "# the melting temperature params need to be preprocessed\n",
    "Tm_params = config[\"Tm_parameters\"][\"shared\"].copy()\n",
    "Tm_params.update(config[\"Tm_parameters\"][\"property_filter\"])\n",
    "Tm_params[\"nn_table\"] = getattr(mt, Tm_params[\"nn_table\"])\n",
    "Tm_params[\"tmm_table\"] = getattr(mt, Tm_params[\"tmm_table\"])\n",
    "Tm_params[\"imm_table\"] = getattr(mt, Tm_params[\"imm_table\"])\n",
    "Tm_params[\"de_table\"] = getattr(mt, Tm_params[\"de_table\"])\n",
    "\n",
    "Tm_correction_param = config[\"Tm_correction_parameters\"][\"shared\"].copy()\n",
    "Tm_correction_param.update(config[\"Tm_correction_parameters\"][\"property_filter\"])\n",
    "\n",
    "# initialize the filters clasees\n",
    "masked_sequences = MaskedSequences()\n",
    "gc_content = GCContent(GC_content_min=config[\"GC_content_min\"], GC_content_max=config[\"GC_content_max\"])\n",
    "melting_temperature = MeltingTemperature(\n",
    "    Tm_min=config[\"Tm_min\"], \n",
    "    Tm_max=config[\"Tm_max\"], \n",
    "    Tm_parameters=Tm_params, \n",
    "    Tm_correction_parameters=Tm_correction_param\n",
    ")\n",
    "padlock_arms = PadlockArms(\n",
    "    min_arm_length=config[\"min_arm_length\"],\n",
    "    max_arm_Tm_dif=config[\"max_arm_Tm_dif\"],\n",
    "    arm_Tm_min=config[\"arm_Tm_min\"],\n",
    "    arm_Tm_max=config[\"arm_Tm_max\"],\n",
    "    Tm_parameters=Tm_params,\n",
    "    Tm_correction_parameters=Tm_correction_param,\n",
    ")\n",
    "# create the list of filters\n",
    "filters = [masked_sequences, gc_content, melting_temperature, padlock_arms]\n",
    "\n",
    "# initialize the property filter class\n",
    "property_filter = PropertyFilter(filters=filters, write_genes_with_insufficient_oligos=config[\"write_removed_genes\"])\n",
    "# filter the database\n",
    "oligo_database = property_filter.apply(oligo_database=oligo_database, n_jobs=config[\"n_jobs\"])\n",
    "# write the intermediate result in a file\n",
    "if config[\"write_intermediate_steps\"]:\n",
    "    oligo_database.write_oligos_DB(format=config[\"file_format\"], dir_oligos_DB=\"property_filter\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specificity filters\n",
    "\n",
    "Generally, in experiments using short oligos one of the main problem that can occur are off-target binding of the designed oligo sequences with unwanted target regions. To avoid this problem we can decide to remove all the oligos that also match regions outside the gene they belong to.\n",
    "\n",
    "The classes in the subpackage `oligo_speificity_filters` detect these oligos using alignment methods such as Blast and Bowtie and remove them from the database. The currently implemeted classes are: `ExactMatches`, `Blastn`, `Bowtie`, `Bowtie2`, `BowtieSeedRegion`. Look at the documentation for detailed information. Those filters are structures in the same way as the property filters. A second class `SpecificityFilter` takes a list of all the filters we want to apply, and applies them sequentially to the `oligos_DB`.  \n",
    "**Note:** the filters are applied in the order they are given as input. Hence, filter with fast computations should be listed first, i.e. apply exact match filter before Blastn filter, to reduce runtime.\n",
    "\n",
    "In addition, alignement methods need a reference fasta file to detect the off-target regions. The `CustomRefereceDB` class provides the possibility to generate this reference region with the method `create_reference_DB`.  \n",
    "**Remark:** it is possible to apply a set of specificity filters with a reference file and a second set with a different reference file. \n",
    "\n",
    "For our pipeline, we will use `ExactMatches`, `Blastn`, `BowtieSeedRegion`. For the `BowtieSeedRegion` filter we need to generate the oligo seed region with `LigationRegionCreation` (look at the documentation to understand what seed region means). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      "  Output files: \"reference_DB_human_GRCh38_NCBI_release_110_transcripts.fna.*.ebwt\"\n",
      "  Line rate: 6 (line is 64 bytes)\n",
      "  Lines per side: 1 (side is 64 bytes)\n",
      "  Offset rate: 5 (one in 32)\n",
      "  FTable chars: 10\n",
      "  Strings: unpacked\n",
      "  Max bucket size: default\n",
      "  Max bucket size, sqrt multiplier: default\n",
      "  Max bucket size, len divisor: 4\n",
      "  Difference-cover sample period: 1024\n",
      "  Endianness: little\n",
      "  Actual local endianness: little\n",
      "  Sanity checking: disabled\n",
      "  Assertions: disabled\n",
      "  Random seed: 0\n",
      "  Sizeofs: void*:8, int:4, long:8, size_t:8\n",
      "Input files DNA, FASTA:\n",
      "  /Users/lisa.barros/projects/GP0002_Oligo_Designer_Toolsuite/Toolsuite/oligo-designer-toolsuite/output/reference/reference_DB_human_GRCh38_NCBI_release_110_transcripts.fna\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:00\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "bmax according to bmaxDivN setting: 1696275\n",
      "Using parameters --bmax 1272207 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 1272207 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 1, merged 6; iterating...\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 1, merged 1; iterating...\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 969299 (target: 1272206)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 7\n",
      "  Reserving size (1272207) for bucket 1\n",
      "  Calculating Z arrays for bucket 1\n",
      "  Entering block accumulator loop for bucket 1:\n",
      "  bucket 1: 10%\n",
      "  bucket 1: 20%\n",
      "  bucket 1: 30%\n",
      "  bucket 1: 40%\n",
      "  bucket 1: 50%\n",
      "  bucket 1: 60%\n",
      "  bucket 1: 70%\n",
      "  bucket 1: 80%\n",
      "  bucket 1: 90%\n",
      "  bucket 1: 100%\n",
      "  Sorting block of length 1033384 for bucket 1\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 1033385 for bucket 1\n",
      "Getting block 2 of 7\n",
      "  Reserving size (1272207) for bucket 2\n",
      "  Calculating Z arrays for bucket 2\n",
      "  Entering block accumulator loop for bucket 2:\n",
      "  bucket 2: 10%\n",
      "  bucket 2: 20%\n",
      "  bucket 2: 30%\n",
      "  bucket 2: 40%\n",
      "  bucket 2: 50%\n",
      "  bucket 2: 60%\n",
      "  bucket 2: 70%\n",
      "  bucket 2: 80%\n",
      "  bucket 2: 90%\n",
      "  bucket 2: 100%\n",
      "  Sorting block of length 1240518 for bucket 2\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 1240519 for bucket 2\n",
      "Getting block 3 of 7\n",
      "  Reserving size (1272207) for bucket 3\n",
      "  Calculating Z arrays for bucket 3\n",
      "  Entering block accumulator loop for bucket 3:\n",
      "  bucket 3: 10%\n",
      "  bucket 3: 20%\n",
      "  bucket 3: 30%\n",
      "  bucket 3: 40%\n",
      "  bucket 3: 50%\n",
      "  bucket 3: 60%\n",
      "  bucket 3: 70%\n",
      "  bucket 3: 80%\n",
      "  bucket 3: 90%\n",
      "  bucket 3: 100%\n",
      "  Sorting block of length 1026083 for bucket 3\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 1026084 for bucket 3\n",
      "Getting block 4 of 7\n",
      "  Reserving size (1272207) for bucket 4\n",
      "  Calculating Z arrays for bucket 4\n",
      "  Entering block accumulator loop for bucket 4:\n",
      "  bucket 4: 10%\n",
      "  bucket 4: 20%\n",
      "  bucket 4: 30%\n",
      "  bucket 4: 40%\n",
      "  bucket 4: 50%\n",
      "  bucket 4: 60%\n",
      "  bucket 4: 70%\n",
      "  bucket 4: 80%\n",
      "  bucket 4: 90%\n",
      "  bucket 4: 100%\n",
      "  Sorting block of length 695047 for bucket 4\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 695048 for bucket 4\n",
      "Getting block 5 of 7\n",
      "  Reserving size (1272207) for bucket 5\n",
      "  Calculating Z arrays for bucket 5\n",
      "  Entering block accumulator loop for bucket 5:\n",
      "  bucket 5: 10%\n",
      "  bucket 5: 20%\n",
      "  bucket 5: 30%\n",
      "  bucket 5: 40%\n",
      "  bucket 5: 50%\n",
      "  bucket 5: 60%\n",
      "  bucket 5: 70%\n",
      "  bucket 5: 80%\n",
      "  bucket 5: 90%\n",
      "  bucket 5: 100%\n",
      "  Sorting block of length 1209289 for bucket 5\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 1209290 for bucket 5\n",
      "Getting block 6 of 7\n",
      "  Reserving size (1272207) for bucket 6\n",
      "  Calculating Z arrays for bucket 6\n",
      "  Entering block accumulator loop for bucket 6:\n",
      "  bucket 6: 10%\n",
      "  bucket 6: 20%\n",
      "  bucket 6: 30%\n",
      "  bucket 6: 40%\n",
      "  bucket 6: 50%\n",
      "  bucket 6: 60%\n",
      "  bucket 6: 70%\n",
      "  bucket 6: 80%\n",
      "  bucket 6: 90%\n",
      "  bucket 6: 100%\n",
      "  Sorting block of length 911841 for bucket 6\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 911842 for bucket 6\n",
      "Getting block 7 of 7\n",
      "  Reserving size (1272207) for bucket 7\n",
      "  Calculating Z arrays for bucket 7\n",
      "  Entering block accumulator loop for bucket 7:\n",
      "  bucket 7: 10%\n",
      "  bucket 7: 20%\n",
      "  bucket 7: 30%\n",
      "  bucket 7: 40%\n",
      "  bucket 7: 50%\n",
      "  bucket 7: 60%\n",
      "  bucket 7: 70%\n",
      "  bucket 7: 80%\n",
      "  bucket 7: 90%\n",
      "  bucket 7: 100%\n",
      "  Sorting block of length 668934 for bucket 7\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:01\n",
      "Returning block of 668935 for bucket 7\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 1651723\n",
      "fchr[G]: 3417971\n",
      "fchr[T]: 5191495\n",
      "fchr[$]: 6785102\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 7410254 bytes to primary EBWT file: reference_DB_human_GRCh38_NCBI_release_110_transcripts.fna.1.ebwt\n",
      "Wrote 848144 bytes to secondary EBWT file: reference_DB_human_GRCh38_NCBI_release_110_transcripts.fna.2.ebwt\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 6785102\n",
      "    bwtLen: 6785103\n",
      "    sz: 1696276\n",
      "    bwtSz: 1696276\n",
      "    lineRate: 6\n",
      "    linesPerSide: 1\n",
      "    offRate: 5\n",
      "    offMask: 0xffffffe0\n",
      "    isaRate: -1\n",
      "    isaMask: 0xffffffff\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 212035\n",
      "    offsSz: 848140\n",
      "    isaLen: 0\n",
      "    isaSz: 0\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 56\n",
      "    sideBwtLen: 224\n",
      "    numSidePairs: 15146\n",
      "    numSides: 30292\n",
      "    numLines: 30292\n",
      "    ebwtTotLen: 1938688\n",
      "    ebwtTotSz: 1938688\n",
      "    reverse: 0\n",
      "Total time for call to driver() for forward index: 00:00:03\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:00\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "bmax according to bmaxDivN setting: 1696275\n",
      "Using parameters --bmax 1272207 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 1272207 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 1, merged 6; iterating...\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 969299 (target: 1272206)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 7\n",
      "  Reserving size (1272207) for bucket 1\n",
      "  Calculating Z arrays for bucket 1\n",
      "  Entering block accumulator loop for bucket 1:\n",
      "  bucket 1: 10%\n",
      "  bucket 1: 20%\n",
      "  bucket 1: 30%\n",
      "  bucket 1: 40%\n",
      "  bucket 1: 50%\n",
      "  bucket 1: 60%\n",
      "  bucket 1: 70%\n",
      "  bucket 1: 80%\n",
      "  bucket 1: 90%\n",
      "  bucket 1: 100%\n",
      "  Sorting block of length 827514 for bucket 1\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 827515 for bucket 1\n",
      "Getting block 2 of 7\n",
      "  Reserving size (1272207) for bucket 2\n",
      "  Calculating Z arrays for bucket 2\n",
      "  Entering block accumulator loop for bucket 2:\n",
      "  bucket 2: 10%\n",
      "  bucket 2: 20%\n",
      "  bucket 2: 30%\n",
      "  bucket 2: 40%\n",
      "  bucket 2: 50%\n",
      "  bucket 2: 60%\n",
      "  bucket 2: 70%\n",
      "  bucket 2: 80%\n",
      "  bucket 2: 90%\n",
      "  bucket 2: 100%\n",
      "  Sorting block of length 1010380 for bucket 2\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 1010381 for bucket 2\n",
      "Getting block 3 of 7\n",
      "  Reserving size (1272207) for bucket 3\n",
      "  Calculating Z arrays for bucket 3\n",
      "  Entering block accumulator loop for bucket 3:\n",
      "  bucket 3: 10%\n",
      "  bucket 3: 20%\n",
      "  bucket 3: 30%\n",
      "  bucket 3: 40%\n",
      "  bucket 3: 50%\n",
      "  bucket 3: 60%\n",
      "  bucket 3: 70%\n",
      "  bucket 3: 80%\n",
      "  bucket 3: 90%\n",
      "  bucket 3: 100%\n",
      "  Sorting block of length 1237895 for bucket 3\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 1237896 for bucket 3\n",
      "Getting block 4 of 7\n",
      "  Reserving size (1272207) for bucket 4\n",
      "  Calculating Z arrays for bucket 4\n",
      "  Entering block accumulator loop for bucket 4:\n",
      "  bucket 4: 10%\n",
      "  bucket 4: 20%\n",
      "  bucket 4: 30%\n",
      "  bucket 4: 40%\n",
      "  bucket 4: 50%\n",
      "  bucket 4: 60%\n",
      "  bucket 4: 70%\n",
      "  bucket 4: 80%\n",
      "  bucket 4: 90%\n",
      "  bucket 4: 100%\n",
      "  Sorting block of length 1119742 for bucket 4\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 1119743 for bucket 4\n",
      "Getting block 5 of 7\n",
      "  Reserving size (1272207) for bucket 5\n",
      "  Calculating Z arrays for bucket 5\n",
      "  Entering block accumulator loop for bucket 5:\n",
      "  bucket 5: 10%\n",
      "  bucket 5: 20%\n",
      "  bucket 5: 30%\n",
      "  bucket 5: 40%\n",
      "  bucket 5: 50%\n",
      "  bucket 5: 60%\n",
      "  bucket 5: 70%\n",
      "  bucket 5: 80%\n",
      "  bucket 5: 90%\n",
      "  bucket 5: 100%\n",
      "  Sorting block of length 550342 for bucket 5\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 550343 for bucket 5\n",
      "Getting block 6 of 7\n",
      "  Reserving size (1272207) for bucket 6\n",
      "  Calculating Z arrays for bucket 6\n",
      "  Entering block accumulator loop for bucket 6:\n",
      "  bucket 6: 10%\n",
      "  bucket 6: 20%\n",
      "  bucket 6: 30%\n",
      "  bucket 6: 40%\n",
      "  bucket 6: 50%\n",
      "  bucket 6: 60%\n",
      "  bucket 6: 70%\n",
      "  bucket 6: 80%\n",
      "  bucket 6: 90%\n",
      "  bucket 6: 100%\n",
      "  Sorting block of length 1183879 for bucket 6\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 1183880 for bucket 6\n",
      "Getting block 7 of 7\n",
      "  Reserving size (1272207) for bucket 7\n",
      "  Calculating Z arrays for bucket 7\n",
      "  Entering block accumulator loop for bucket 7:\n",
      "  bucket 7: 10%\n",
      "  bucket 7: 20%\n",
      "  bucket 7: 30%\n",
      "  bucket 7: 40%\n",
      "  bucket 7: 50%\n",
      "  bucket 7: 60%\n",
      "  bucket 7: 70%\n",
      "  bucket 7: 80%\n",
      "  bucket 7: 90%\n",
      "  bucket 7: 100%\n",
      "  Sorting block of length 855344 for bucket 7\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 855345 for bucket 7\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 1651723\n",
      "fchr[G]: 3417971\n",
      "fchr[T]: 5191495\n",
      "fchr[$]: 6785102\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 7410254 bytes to primary EBWT file: reference_DB_human_GRCh38_NCBI_release_110_transcripts.fna.rev.1.ebwt\n",
      "Wrote 848144 bytes to secondary EBWT file: reference_DB_human_GRCh38_NCBI_release_110_transcripts.fna.rev.2.ebwt\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 6785102\n",
      "    bwtLen: 6785103\n",
      "    sz: 1696276\n",
      "    bwtSz: 1696276\n",
      "    lineRate: 6\n",
      "    linesPerSide: 1\n",
      "    offRate: 5\n",
      "    offMask: 0xffffffe0\n",
      "    isaRate: -1\n",
      "    isaMask: 0xffffffff\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 212035\n",
      "    offsSz: 848140\n",
      "    isaLen: 0\n",
      "    isaSz: 0\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 56\n",
      "    sideBwtLen: 224\n",
      "    numSidePairs: 15146\n",
      "    numSides: 30292\n",
      "    numLines: 30292\n",
      "    ebwtTotLen: 1938688\n",
      "    ebwtTotSz: 1938688\n",
      "    reverse: 0\n",
      "Total time for backward call to driver() for mirror index: 00:00:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# reads processed: 882\n",
      "# reads with at least one alignment: 882 (100.00%)\n",
      "# reads that failed to align: 0 (0.00%)\n",
      "Reported 32902 alignments\n",
      "# reads processed: 126\n",
      "# reads with at least one alignment: 126 (100.00%)\n",
      "# reads that failed to align: 0 (0.00%)\n",
      "Reported 1791 alignments\n",
      "# reads processed: 2196\n",
      "# reads with at least one alignment: 2196 (100.00%)\n",
      "# reads that failed to align: 0 (0.00%)\n",
      "Reported 63683 alignments\n",
      "# reads processed: 11276\n",
      "# reads with at least one alignment: 11276 (100.00%)\n",
      "# reads that failed to align: 0 (0.00%)\n",
      "Reported 304029 alignments\n",
      "# reads processed: 1211\n",
      "# reads with at least one alignment: 1211 (100.00%)\n",
      "# reads that failed to align: 0 (0.00%)\n",
      "Reported 50523 alignments\n",
      "# reads processed: 0\n",
      "# reads with at least one alignment: 0 (0.00%)\n",
      "# reads that failed to align: 0 (0.00%)\n",
      "No alignments\n",
      "# reads processed: 0\n",
      "# reads with at least one alignment: 0 (0.00%)\n",
      "# reads that failed to align: 0 (0.00%)\n",
      "No alignments\n",
      "# reads processed: 0\n",
      "# reads with at least one alignment: 0 (0.00%)\n",
      "# reads that failed to align: 0 (0.00%)\n",
      "No alignments\n",
      "# reads processed: 0\n",
      "# reads with at least one alignment: 0 (0.00%)\n",
      "# reads that failed to align: 0 (0.00%)\n",
      "No alignments\n",
      "# reads processed: 0\n",
      "# reads with at least one alignment: 0 (0.00%)\n",
      "# reads that failed to align: 0 (0.00%)\n",
      "No alignments\n"
     ]
    }
   ],
   "source": [
    "from oligo_designer_toolsuite.database import CustomReferenceDB, NcbiReferenceDB, EnsemblReferenceDB\n",
    "from oligo_designer_toolsuite.oligo_specificity_filter import (\n",
    "    SpecificityFilter,\n",
    "    ExactMatches,\n",
    "    LigationRegionCreation,\n",
    "    BowtieSeedRegion,\n",
    "    Blastn,\n",
    ")\n",
    "\n",
    "dir_specificity = os.path.join(dir_output, \"specificity_temporary\") # folder where the temporary files will be written\n",
    "\n",
    "# generate the reference\n",
    "reference_database = CustomReferenceDB(\n",
    "    species=oligo_database.species,\n",
    "    genome_assembly=oligo_database.genome_assembly,\n",
    "    annotation_release=oligo_database.annotation_release,\n",
    "    files_source=oligo_database.files_source,\n",
    "    annotation_file=oligo_database.annotation_file,\n",
    "    sequence_file=oligo_database.sequence_file,\n",
    "    dir_output=dir_output\n",
    ")\n",
    "reference_database.create_reference_DB(block_size=config[\"block_size\"]) # leave the standard parameters\n",
    "\n",
    "# intialize the filter classes\n",
    "exact_mathces = ExactMatches(dir_specificity=dir_specificity)\n",
    "seed_ligation = LigationRegionCreation(ligation_region_size=config[\"ligation_region_size\"])\n",
    "seed_region = BowtieSeedRegion(dir_specificity=dir_specificity, seed_region_creation=seed_ligation)\n",
    "blastn = Blastn(\n",
    "    dir_specificity=dir_specificity, \n",
    "    word_size=config[\"word_size\"],\n",
    "    percent_identity=config[\"percent_identity\"],\n",
    "    coverage=config[\"coverage\"],\n",
    ")\n",
    "filters = [exact_mathces, seed_region, blastn]\n",
    "\n",
    "# initialize the specificity filter class\n",
    "specificity_filter = SpecificityFilter(filters=filters, write_genes_with_insufficient_oligos=config[\"write_removed_genes\"])\n",
    "# filte r the database\n",
    "oligo_database = specificity_filter.apply(oligo_database=oligo_database, reference_database=reference_database, n_jobs=config[\"n_jobs\"])\n",
    "# write the intermediate result\n",
    "if config[\"write_intermediate_steps\"]:\n",
    "    oligo_database.write_oligos_DB(format=config[\"file_format\"], dir_oligos_DB=\"specificity_filter\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oligoset generation\n",
    "\n",
    "In the next step of the pipeline the oligos will be choosen according to their theoretical efficiency in the experiment scope (e.g. how well they bind to the target in the DNA). Each oligo will receive a score computed by a class that inherits from `OligoScoringBase`. Later, the sequences will be organized in sets and a class inheriting from `SetScoringBase` will give a general efficiency score to the set. At the end the best sets will be selected and sored.\n",
    "\n",
    "It is required that the each array of sequences  contains oligos that do not overlap. In fact, if two oligos were overlapping, they would compete to bind to the same section of DNA and their efficiency would drop significantly. Therefore, it is extremely important to consider only sets of non-overlapping sequences.\n",
    "\n",
    "The class `OligosetGenerator` takes the scoring strategies and tries to find, among all the feasible non-overlapping sets of oligos, the sets with the best efficiency scores. These sets will be save in a pandas DataFrame with the following structure:\n",
    "\n",
    "\n",
    " oligoset_id | oligo_0  | oligo_1  | oligo_2  |  ...  | oligo_n  | set_score_1 | set_score_2 |  ...  \n",
    "------------ | -------- | -------- | -------- | ----- | -------- | ----------- | ----------- | ------:\n",
    " 0           | AGRN_184 | AGRN_133 | AGRN_832 |  ...  | AGRN_706 | 0.3445      | 1.2332      |  ...  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oligo_designer_toolsuite.oligo_efficiency import(\n",
    "    PadlockOligoScoring,\n",
    "    PadlockSetScoring,\n",
    ")\n",
    "from oligo_designer_toolsuite.oligo_selection import OligosetGenerator, padlock_heuristic_selection\n",
    "\n",
    "# initialize the scoring classes\n",
    "oligos_scoring = PadlockOligoScoring(\n",
    "    Tm_min=config[\"Tm_min\"],\n",
    "    Tm_opt=config[\"Tm_opt\"],\n",
    "    Tm_max=config[\"Tm_max\"],\n",
    "    GC_content_min=config[\"GC_content_min\"],\n",
    "    GC_content_opt=config[\"GC_content_opt\"],\n",
    "    GC_content_max=config[\"GC_content_max\"],\n",
    "    Tm_weight=config[\"Tm_weight\"],\n",
    "    GC_weight=config[\"GC_weight\"],\n",
    ")\n",
    "set_scoring = PadlockSetScoring()\n",
    "\n",
    "# initialize the oligoset generator class\n",
    "oligoset_generator = OligosetGenerator(\n",
    "    oligoset_size=config[\"oligoset_size\"], \n",
    "    min_oligoset_size=config[\"min_oligoset_size\"],\n",
    "    oligos_scoring=oligos_scoring,\n",
    "    set_scoring=set_scoring,\n",
    "    heurustic_selection=padlock_heuristic_selection,\n",
    "    write_genes_with_insufficient_oligos=config[\"write_removed_genes\"]\n",
    ")\n",
    "\n",
    "# generate the oligoset\n",
    "oligo_database = oligoset_generator.apply(oligo_database=oligo_database, n_sets=config[\"n_sets\"], n_jobs=config[\"n_jobs\"])\n",
    "# write the intermediate result\n",
    "if config[\"write_intermediate_steps\"]:\n",
    "    oligo_database.write_oligosets(dir_oligosets=\"oligosets\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last step\n",
    "\n",
    "Once the best oligosets are generated each experiment design might require (or not) an addtional step. In the case of the Padlock oligo designer the last step consists in designing the final padlock oligo sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oligo_designer_toolsuite.experiment_specific import PadlockSequenceDesigner\n",
    "\n",
    "# preprocessing of themelting temperature parameters\n",
    "Tm_params = config[\"Tm_parameters\"][\"shared\"].copy()\n",
    "Tm_params.update(config[\"Tm_parameters\"][\"detection_oligo\"])\n",
    "Tm_params[\"nn_table\"] = getattr(mt, Tm_params[\"nn_table\"])\n",
    "Tm_params[\"tmm_table\"] = getattr(mt, Tm_params[\"tmm_table\"])\n",
    "Tm_params[\"imm_table\"] = getattr(mt, Tm_params[\"imm_table\"])\n",
    "Tm_params[\"de_table\"] = getattr(mt, Tm_params[\"de_table\"])\n",
    "\n",
    "Tm_correction_param = config[\"Tm_correction_parameters\"][\"shared\"].copy()\n",
    "Tm_correction_param.update(config[\"Tm_correction_parameters\"][\"detection_oligo\"])\n",
    "\n",
    "# initilize the padlock sequence designer class\n",
    "padlock_sequence_designer = PadlockSequenceDesigner(\n",
    "    detect_oligo_length_min=config[\"detect_oligo_length_min\"],\n",
    "    detect_oligo_length_max=config[\"detect_oligo_length_max\"],\n",
    "    detect_oligo_Tm_opt=config[\"detect_oligo_Tm_opt\"],\n",
    "    Tm_parameters=Tm_params,\n",
    "    Tm_correction_parameters=Tm_correction_param\n",
    ")\n",
    "# generate the padlock sequence\n",
    "padlock_sequence_designer.design_padlocks(database=oligo_database)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ODT",
   "language": "python",
   "name": "odt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ff94d65164c54a2126c7dc0869520902e3a005d5dddb9f8c3667ad0fd9373c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
